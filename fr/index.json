[{"authors":["scott"],"categories":null,"content":" A propos Je suis chercheur en neurosciences cognitives utilisant actuellement des techniques de neuroimagerie chez des animaux de ferme pour étudier : 1) l\u0026rsquo;influence de l\u0026rsquo;environnement précoce sur le développement comportemental et neurobiologique et 2) la perception sociale et ses mechanismes neuronaux sous jacent.\nDepuis novembre 2017, je suis Chargé de Recherche à l\u0026rsquo;Unité Mixte de Recherche PRC (Physiologie de la Reproduction et des Comportements) à Nouzilly, France.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"fr","lastmod":1561362441,"objectID":"00ce5bf736bf5b73096e222f166bef2e","permalink":"https://scott-love.github.io/fr/authors/scott/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/fr/authors/scott/","section":"authors","summary":"A propos Je suis chercheur en neurosciences cognitives utilisant actuellement des techniques de neuroimagerie chez des animaux de ferme pour étudier : 1) l\u0026rsquo;influence de l\u0026rsquo;environnement précoce sur le développement comportemental et neurobiologique et 2) la perception sociale et ses mechanismes neuronaux sous jacent.\nDepuis novembre 2017, je suis Chargé de Recherche à l\u0026rsquo;Unité Mixte de Recherche PRC (Physiologie de la Reproduction et des Comportements) à Nouzilly, France.","tags":null,"title":"Scott Love","type":"authors"},{"authors":["Scott A. Love","Karin Petrini","Cyril R. Pernet","Marianne Latinus","Frank E. Pollick"],"categories":null,"content":"","date":1531221530,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"cd0d45b0cd6b43ae19ec70f4a0cc26f4","permalink":"https://scott-love.github.io/fr/publication/love2018/","publishdate":"2018-07-10T12:18:50+01:00","relpermalink":"/fr/publication/love2018/","section":"publication","summary":"Multisensory processing is a core perceptual capability, and the need to understand its neural bases provides a fundamental problem in the study of brain function. Both synchrony and temporal order judgments are commonly used to investigate synchrony perception between different sensory cues and multisensory perception in general. However, extensive behavioral evidence indicates that these tasks do not measure identical perceptual processes. Here we used functional magnetic resonance imaging to investigate how behavioral differences between the tasks are instantiated as neural differences. As these neural differences could manifest at either the sustained (task/state-related) and/or transient (event-related) levels of processing, a mixed block/event-related design was used to investigate the neural response of both time-scales. Clear differences in both sustained and transient BOLD responses were observed between the two tasks, consistent with behavioral differences indeed arising from overlapping but divergent neural mechanisms. Temporalorder judgments, butnot synchrony judgments, required transient activation in several left hemisphere regions, which may reflect increased task demands caused by an extra stage of processing. Our results highlight that multisensory integrationmechanisms can be task dependent, which, in particular, has implications for the study of atypical temporal processing in clinical populations.","tags":[],"title":"Overlapping but Divergent Neural Correlates Underpinning Audiovisual Synchrony and Temporal Order Judgments","type":"publication"},{"authors":null,"categories":null,"content":"ADD TEXT HERE\nFunded by INRA Department PHASE Crédits Incitatifs 2018 (19000 euros)\n","date":1518109698,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"938eb237d838a317cd3a0ecd886d9483","permalink":"https://scott-love.github.io/fr/project/phenomathyp/","publishdate":"2018-02-08T18:08:18+01:00","relpermalink":"/fr/project/phenomathyp/","section":"project","summary":"ADD TEXT HERE\nFunded by INRA Department PHASE Crédits Incitatifs 2018 (19000 euros)","tags":["ovine","mri","brain","maturation","pituitary gland","texture analysis"],"title":"Phenomathyp","type":"project"},{"authors":null,"categories":null,"content":"ADD TEXT HERE\n","date":1518109580,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"ebbf0cb46d0bab214d4494fff672df33","permalink":"https://scott-love.github.io/fr/project/neuro2co/","publishdate":"2018-02-08T18:06:20+01:00","relpermalink":"/fr/project/neuro2co/","section":"project","summary":"ADD TEXT HERE","tags":["ovine","mri","brain","maturation"],"title":"Neuro2co","type":"project"},{"authors":null,"categories":null,"content":"ADD TEXT HERE\n","date":1518109421,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"06e83df2cc75116ce2e50abaa23199c7","permalink":"https://scott-love.github.io/fr/project/prebiostress/","publishdate":"2018-02-08T18:03:41+01:00","relpermalink":"/fr/project/prebiostress/","section":"project","summary":"ADD TEXT HERE","tags":["ovine","mri","brain","maturation"],"title":"Prebiostress","type":"project"},{"authors":null,"categories":[],"content":" A quick(ish) tutorial on how I setup my personal but work related website using Git(Hub), Hugo and Academic on a Mac. This is a fairly simple and completely free way to host an academic personal website. The process came from two main sources: 1. The documentation for Academic and; 2. A blog post by George Cushen.\nSo what did I do:  I already had Git installed and you will need it too.\n Then install Hugo. There are several options but I used the Tarball method. Full details can be found here but here is a brief description.\n Chose install location (e.g., /usr/local/bin), in your executable PATH. Download the latest Tarball for your system to the Downloads folder. Install Hugo in your chosen location.\ncd /usr/local/bin # CHOSEN_INSTALL_LOCATION # extract the tarball tar -xvzf ~/Downloads/hugo_X.Y_osx-64bit.tgz # verify that it runs ./hugo version   Fork the Academic Kickstart repository to your GitHub account.\n Login to your GitHub account. Click here or search for sourcethemes/academic-kickstart inside GitHub. Click on the fork icon towards the top right of the screen.   Clone the repository onto your local system. Note that you need to change \u0026lt;USERNAME\u0026gt; to your GitHub username and that My_Website can be changed to any name you want.\ngit clone https://github.com/\u0026lt;USERNAME\u0026gt;/academic-kickstart.git My_Website  Initialise the theme.\ncd My_Website git submodule update --init --recursive  Create your GitHub website repository.\n On your GitHub page click the “+” icon in the top right corner and choose “New Repository”. Repository name = \u0026lt;USERNAME\u0026gt;.github.io.   Add your \u0026lt;USERNAME\u0026gt;.github.io repository into a submodule folder named public.\ngit submodule add https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public  Add to the local git repository then push to the remote repository.\ngit add . git commit -m \u0026quot;initial commit\u0026quot; git push -u origin master  Run Hugo to create the HTML for the site.\nhugo server  View the built site in your browser (http://localhost:1313/) but note this is only a local copy and not visible to others.\n Upload the built site to Github for everyone to see @ \u0026lt;USERNAME\u0026gt;.github.io. Note it will take 2 or 3 minutes to be viewable.\ngit add . git commit -m \u0026quot;build website\u0026quot; git push -u origin master   Adding content: OK, the website is built it\u0026rsquo;s time to add content. You can make additions/changes to your website and check out the results before deploying to your actual site.\nA good place to start is the config.toml file in the main directory of your site. It contains a bunch of key-value pairs. change the title value to the title of your website, e.g., your name. Then run Hugo server to create the HTML for the site.\nhugo server  View the local copy of the built site in your browser (http://localhost:1313/). Leave the server running and any other changes you make will be automatically visible in local site just created. Go through all the key-value pairs and change, edit or remove any that you want.\nGithub version control: The Host on GitHub page from Hugo outlines a nice way to store all the files of your site on GitHub.\nUpdating the website content: Using the GitHub method above I do not keep a local copy of any of the files. Here is the process I follow to update the website content.\ngit clone https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;YOUR-PROJECT\u0026gt; cd \u0026lt;YOUR-PROJECT\u0026gt;/ git rm -r public  At this point you can make changes to the content of your website and push those to the remote repository.\ngit add . git commit -m \u0026quot;updating content\u0026quot; git push origin master  However, this only keeps the content in synchrony with the remote repository it does not update the website.\ngit submodule add https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public ./deploy.sh \u0026quot;comment\u0026quot;  The contents of deploy.sh can be seen here\nUpdate academic-kickstart version: I have found updating the academic-kickstart version and subsequently the website to be tricky! Follow the instructions here. However, I was only ever able to update using the ZIP there. Uninstall the current version by deleting the contents of the \u0026ldquo;themes/academic/\u0026rdquo; folder inside \u0026lt;YOUR-PROJECT\u0026gt; and replacing it with the downloaded files.\nAt this point you still need to follow the release notes and update your content and config files to take into account the \u0026ldquo;Breaking changes\u0026rdquo;. If you have jumped a few versions you will need to do this in sequence for each version change (e.g. v3.1 to v3.2 before doing v3.2 to v3.3)\n","date":1518024697,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"33d571ee6822f89daf3f4badd846a8de","permalink":"https://scott-love.github.io/fr/post/create-this-site/","publishdate":"2018-02-07T18:31:37+01:00","relpermalink":"/fr/post/create-this-site/","section":"post","summary":"A quick(ish) tutorial on how I setup my personal but work related website using Git(Hub), Hugo and Academic on a Mac. This is a fairly simple and completely free way to host an academic personal website. The process came from two main sources: 1. The documentation for Academic and; 2. A blog post by George Cushen.\nSo what did I do:  I already had Git installed and you will need it too.","tags":[],"title":"Git(Hub), Hugo and Academic","type":"post"},{"authors":["Maureen Fontaine","Scott A. Love","Marianne Latinus"],"categories":null,"content":"","date":1486484582,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"00e8ef540d13327032d1467aeea9b7ff","permalink":"https://scott-love.github.io/fr/publication/fontaine2017/","publishdate":"2017-02-07T17:23:02+01:00","relpermalink":"/fr/publication/fontaine2017/","section":"publication","summary":"The ability to recognize an individual from their voice is a widespread ability with a long evolutionary history. Yet, the perceptual representation of familiar voices is ill-defined. In two experiments, we explored the neuropsychological processes involved in the perception of voice identity. We specifically explored the hypothesis that familiar voices (trained-to-familiar (Experiment 1), and famous voices (Experiment 2)) are represented as a whole complex pattern, well approximated by the average of multiple utterances produced by a single speaker. In experiment 1, participants learned three voices over several sessions, and performed a three-alternative forced-choice identification task on original voice samples and several “speaker averages,” created by morphing across varying numbers of different vowels (e.g., [a] and [i]) produced by the same speaker. In experiment 2, the same participants performed the same task on voice samples produced by familiar speakers. The two experiments showed that for famous voices, but not for trained-to-familiar voices, identification performance increased and response times decreased as a function of the number of utterances in the averages. This study sheds light on the perceptual representation of familiar voices, and demonstrates the power of average in recognizing familiar voices. The speaker average captures the unique characteristics of a speaker, and thus retains the information essential for recognition; it acts as a prototype of the speaker.","tags":["voice","identity","experience"],"title":"Familiarity and Voice Representation: From Acoustic-Based Representation to Voice Averages","type":"publication"},{"authors":["Damian Marie","Muriel Roth","Romain Lacoste","Bruno Nazarian","Bertello Alice","Jean-Luc Anton","William D. Hopkins","Konstantina Margiotoudi","Scott A. Love","Adrien Meguerditchian"],"categories":null,"content":"","date":1486484582,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"b9feef73e662ad74dcf22ee0b37f16c5","permalink":"https://scott-love.github.io/fr/publication/marie2017/","publishdate":"2017-02-07T17:23:02+01:00","relpermalink":"/fr/publication/marie2017/","section":"publication","summary":"The planum temporale (PT) is a critical region of the language functional network in the human brain showing a striking size asymmetry toward the left hemisphere. Historically considered as a structural landmark of the left-brain specialization for language, a similar anatomical bias has been described in great apes but never in monkeys—indicating that this brain landmark might be unique to Hominidae evolution. In the present in vivo magnetic resonance imaging study, we show clearly for the first time in a nonhominid primate species, an Old World monkey, a left size predominance of the PT among 96 olive baboons (Papio anubis), using manual delineation of this region in each individual hemisphere. This asymmetric distribution was quasi-identical to that found originally in humans. Such a finding questions the relationship between PT asymmetry and the emergence of language, indicating that the origin of this cerebral specialization could be much older than previously thought, dating back, not to the Hominidae, but rather to the Catarrhini evolution at the common ancestor of humans, great apes and Old World monkeys, 30–40 million years ago.","tags":[],"title":"Left Brain Asymmetry of the Planum Temporale in a Nonhominid Primate: Redefining the Origin of Brain Specialization for Language","type":"publication"},{"authors":null,"categories":null,"content":"See this poster on Figshare for some more details.\n","date":1484174506,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"c97427401757157a2f237781e1a58190","permalink":"https://scott-love.github.io/fr/project/sheep-cortex/","publishdate":"2017-01-11T23:41:46+01:00","relpermalink":"/fr/project/sheep-cortex/","section":"project","summary":"See this poster on Figshare for some more details.","tags":["ovine","anatomy","cortex","mri"],"title":"Sheep Cortex","type":"project"},{"authors":["Scott A. Love","Damien Marie","Muriel Roth","Romain Lacoste","Bruno Nazarian","Alice Bertello","Olivier Coulon","Jean-Luc Anton","Adrien Meguerditchian"],"categories":null,"content":"","date":1457701968,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"fdb29704314fb6ccc0597e4853eddc5e","permalink":"https://scott-love.github.io/fr/publication/love2016/","publishdate":"2016-03-11T14:12:48+01:00","relpermalink":"/fr/publication/love2016/","section":"publication","summary":"The baboon (Papio) brain is a remarkable model for investigating the brain. The current work aimed at creating a population-average baboon (Papio anubis) brain template and its left/right hemisphere symmetric version from a large sample of T1-weighted magnetic resonance images collected from 89 individuals. Averaging the prior probability maps output during the segmentation of each individual also produced the first baboon brain tissue probability maps for grey matter, white matter and cerebrospinal fluid. The templates and the tissue probability maps were created using state-of-the-art, freely available software tools and are being made freely and publicly available: http://www.nitrc.org/projects/haiko89/. It is hoped that these images will aid neuroimaging research of the baboon by, for example, providing a modern, high quality normalization target and accompanying standardized coordinate system as well as probabilistic priors that can be used during tissue segmentation.","tags":[],"title":"The average baboon brain: MRI templates and tissue probability maps from 89 individuals","type":"publication"},{"authors":["Christophe Destrieux","Louis Marie Terrier","Frédéric Andersson","Scott A. Love","Jean-Philippe Cottier","Henri Duvernoy","Stéphane Velut","Kevin Janot","Ilyess Zemmoura"],"categories":null,"content":"","date":1454936810,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"66173d1f36099c68b092bdd2b1745d90","permalink":"https://scott-love.github.io/fr/publication/destrieux2016/","publishdate":"2016-02-08T14:06:50+01:00","relpermalink":"/fr/publication/destrieux2016/","section":"publication","summary":"The precise sulcogyral localization of cortical lesions is mandatory to improve communication between practitioners and to predict and prevent post-operative deficits. This process, which assumes a good knowledge of the cortex anatomy and a systematic analysis of images, is, nevertheless, sometimes neglected in the neurological and neurosurgical training. This didactic paper proposes a brief overview of the sulcogyral anatomy, using conventional MR-slices, and also reconstructions of the cortical surface after a more or less extended inflation process. This method simplifies the cortical anatomy by removing part of the cortical complexity induced by the folding process, and makes it more understandable. We then reviewed several methods for localizing cortical structures, and proposed a three-step identification: after localizing the lateral, medial or ventro-basal aspect of the hemisphere (step 1), the main interlobar sulci were located to limit the lobes (step 2). Finally, intralobar sulci and gyri were identified (step 3) thanks to the same set of rules. This paper does not propose any new identification method but should be regarded as a set of practical guidelines, useful in daily clinical practice, for detecting the main sulci and gyri of the human cortex.","tags":["anatomy","brain","mri","cortex"],"title":"A practical guide for the identification of major sulcogyral structures of the human cortex","type":"publication"},{"authors":["Marianne Latinus","Scott A. Love","Alejandra Rossi","Francisco J. Parada","Lisa Huang","Laurence Conty","Nathalie George","Karin James","Aina Puce"],"categories":null,"content":"","date":1423409366,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"8a278672acc172a9061e776fb3fe37b2","permalink":"https://scott-love.github.io/fr/publication/latinus2015/","publishdate":"2015-02-08T16:29:26+01:00","relpermalink":"/fr/publication/latinus2015/","section":"publication","summary":"Gaze direction, a cue of both social and spatial attention, is known to modulate early neural responses to faces e.g. N170. However, findings in the literature have been inconsistent, likely reflecting differences in stimulus characteristics and task requirements. Here, we investigated the effect of task on neural responses to dynamic gaze changes: away and toward transitions (resulting or not in eye contact). Subjects performed, in random order, social (away/toward them) and non-social (left/right) judgment tasks on these stimuli. Overall, in the non-social task, results showed a larger N170 to gaze aversion than gaze motion toward the observer. In the social task, however, this difference was no longer present in the right hemisphere, likely reflecting an enhanced N170 to gaze motion toward the observer. Our behavioral and event-related potential data indicate that performing social judgments enhances saliency of gaze motion toward the observer, even those that did not result in gaze contact. These data and that of previous studies suggest two modes of processing visual information: a 'default mode' that may focus on spatial information; a 'socially aware mode' that might be activated when subjects are required to make social judgments. The exact mechanism that allows switching from one mode to the other remains to be clarified.","tags":[],"title":"Social decisions affect neural activity to perceived dynamic gaze","type":"publication"},{"authors":["Aina Puce","Marianne Latinus","Alejandra Rossi","Elizabeth DaSilva","Francisco Parada","Scott Love","Arian Ashourvan","Swapnaa Jayaraman"],"categories":null,"content":"","date":1423409001,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"9bf6a3f340f4270eda3dd9378755e52d","permalink":"https://scott-love.github.io/fr/publication/puce2015/","publishdate":"2015-02-08T16:23:21+01:00","relpermalink":"/fr/publication/puce2015/","section":"publication","summary":"In this chapter we focus on the neural processes that occur in the mature healthy human brain in response to evaluating another’s social attention. We first examine the brain’s sensitivity to gaze direction of others, social attention (as typically indicated by gaze contact), and joint attention. Brain regions such as the superior temporal sulcus (STS), the amygdala, and the fusiform gyrus have been previously demonstrated to be sensitive to gaze changes, most frequently with functional magnetic resonance imaging (fMRI). Neurophysiological investigations, using electroencephalography (EEG) and magnetoencephalography (MEG), have identified event-related potentials (ERPs) such as the N170 that are sensitive to changes in gaze direction and head direction. We advance a putative model that explains findings relating to the neurophysiology of social attention , based mainly on our studies. This model proposes two brain modes of social information processing—a nonsocial “Default” mode and a social mode that we have named “Socially Aware”. In Default mode, there is an internal focus on executing actions to achieve our goals, as evident in studies in which passive viewing or tasks involving nonsocial judgments have been used. In contrast, Socially Aware mode is active when making explicit social judgments. Switching between these two modes is rapid and can occur via either top-down or bottom-up routes. From a different perspective, most of the literature, including our own studies, has focused on social attention phenomena as experienced from the first-person perspective, i.e., gaze changes or social attention directed at, or away from, the observer. However, in daily life we are actively involved in observing social interactions between others, where their social attention focus may not include us, or their gaze may not meet ours. Hence, changes in eye gaze and social attention are experienced from the third-person perspective. This area of research is still fairly small, but nevertheless important in the study of social and joint attention, and we discuss this very small literature briefly at the end of the chapter. We conclude the chapter with some outstanding questions, which are aimed at the main knowledge gaps in the literature.","tags":[],"title":"Neural Bases for Social Attention in Healthy Humans","type":"publication"},{"authors":["Phil McAleer","Frank E. Pollick","Scott A. Love","Frances Crabbe","Jeffrey M. Zacks"],"categories":null,"content":"","date":1391873629,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"573509d166d850a7ff2fa6f0af98b764","permalink":"https://scott-love.github.io/fr/publication/mcaleer2014/","publishdate":"2014-02-08T16:33:49+01:00","relpermalink":"/fr/publication/mcaleer2014/","section":"publication","summary":"It has been proposed that we make sense of the movements of others by observing fluctuations in the kinematic properties of their actions. At the neural level, activity in the human motion complex (hMT+) and posterior superior temporal sulcus (pSTS) has been implicated in this relationship. However, previous neuroimaging studies have largely utilized brief, diminished stimuli, and the role of relevant kinematic parameters for the processing of human action remains unclear. We addressed this issue by showing extended-duration natural displays of an actor engaged in two common activities, to 12 participants in an fMRI study under passive viewing conditions. Our region-of-interest analysis focused on three neural areas (hMT+, pSTS, and fusiform face area) and was accompanied by a whole-brain analysis. The kinematic properties of the actor, particularly the speed of body part motion and the distance between body parts, were related to activity in hMT+ and pSTS. Whole-brain exploratory analyses revealed additional areas in posterior cortex, frontal cortex, and the cerebellum whose activity was related to these features. These results indicate that the kinematic properties of peoples' movements are continually monitored during everyday activity as a step to determining actions and intent.","tags":[],"title":"The role of kinematics in cortical regions for continuous human motion perception","type":"publication"},{"authors":["Scott A. Love","Karin Petrini","Adam Cheng","Frank E. Pollick"],"categories":null,"content":"","date":1360338864,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"e036dff4eb667ed21b0297fb6f66da97","permalink":"https://scott-love.github.io/fr/publication/love2013/","publishdate":"2013-02-08T16:54:24+01:00","relpermalink":"/fr/publication/love2013/","section":"publication","summary":"Synchrony judgments involve deciding whether cues to an event are in synch or out of synch, while temporal order judgments involve deciding which of the cues came first. When the cues come from different sensory modalities these judgments can be used to investigate multisensory integration in the temporal domain. However, evidence indicates that that these two tasks should not be used interchangeably as it is unlikely that they measure the same perceptual mechanism. The current experiment further explores this issue across a variety of different audiovisual stimulus types. Participants were presented with 5 audiovisual stimulus types, each at 11 parametrically manipulated levels of cue asynchrony. During separate blocks, participants had to make synchrony judgments or temporal order judgments. For some stimulus types many participants were unable to successfully make temporal order judgments, but they were able to make synchrony judgments. The mean points of subjective simultaneity for synchrony judgments were all video-leading, while those for temporal order judgments were all audio-leading. In the within participants analyses no correlation was found across the two tasks for either the point of subjective simultaneity or the temporal integration window. Stimulus type influenced how the two tasks differed; nevertheless, consistent differences were found between the two tasks regardless of stimulus type. Therefore, in line with previous work, we conclude that synchrony and temporal order judgments are supported by different perceptual mechanisms and should not be interpreted as being representative of the same perceptual process.","tags":[],"title":"A Psychophysical Investigation of Differences Between Synchrony and Temporal Order Judgments","type":"publication"},{"authors":["Corinne Jola","Phil McAleer","Marie-Hélène Grosbras","Scott A. Love","Gordon Morison","Frank E. Pollick"],"categories":null,"content":"","date":1360338582,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"3e91525126669a0b675a308c2c7f47b2","permalink":"https://scott-love.github.io/fr/publication/jola2013/","publishdate":"2013-02-08T16:49:42+01:00","relpermalink":"/fr/publication/jola2013/","section":"publication","summary":"The superior temporal sulcus (STS) and gyrus (STG) are commonly identified to be functionally relevant for multisensory integration of audiovisual (AV) stimuli. However, most neuroimaging studies on AV integration used stimuli of short duration in explicit evaluative tasks. Importantly though, many of our AV experiences are of a long duration and ambiguous. It is unclear if the enhanced activity in audio, visual, and AV brain areas would also be synchronised over time across subjects when they are exposed to such multisensory stimuli. We used intersubject correlation to investigate which brain areas are synchronised across novices for uni- and multisensory versions of a 6-min 26-s recording of an unfamiliar, unedited Indian dance recording (Bharatanatyam). In Bharatanatyam, music and dance are choreographed together in a highly intermodal-dependent manner. Activity in the middle and posterior STG was significantly correlated between subjects and showed also significant enhancement for AV integration when the functional magnetic resonance signals were contrasted against each other using a general linear model conjunction analysis. These results extend previous studies by showing an intermediate step of synchronisation for novices: while there was a consensus across subjects' brain activity in areas relevant for unisensory processing and AV integration of related audio and visual stimuli, we found no evidence for synchronisation of higher level cognitive processes, suggesting these were idiosyncratic.","tags":["dance","fmri","multisensory"],"title":"Uni- and multisensory brain areas are synchronised across spectators when watching unedited dance recordings","type":"publication"},{"authors":["Phil McAleer","Scott A. Love"],"categories":null,"content":"","date":1360338114,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"ff3727217f22162c1f463e29a983c353","permalink":"https://scott-love.github.io/fr/publication/mcaleer2013/","publishdate":"2013-02-08T16:41:54+01:00","relpermalink":"/fr/publication/mcaleer2013/","section":"publication","summary":"Typically, the actions of agents in classical animacy displays are synthetically created, thus forming artificial displays of biological movement. Therefore, the link between the motion in animacy displays and that of actual biological motion is unclear. In this chapter we will look at work being done to clarify this relationship. We will first discuss a modern approach to the creation of animacy displays whereby fullvideo displays of human interactions are reduced into simple animacy displays; this results in animate shapes whose motions are directly derived from human actions. Second, we will review what is known about the ability of typically developed adults and people with autism spectrum disorders to perceive the intentionality within these displays. Finally, we will explore the effects that motion parameters such as speed and acceleration, measured directly from original human actions, have on the perception of intent; fMRI studies that connect neural networks to motion parameters, and the resultant perception of animacy and intention, will also be examined.","tags":[],"title":"Perceiving intention in animacy displays created from human motion","type":"publication"},{"authors":["Pierre Maurage","Scott A. Love","Fabien D'Hondt"],"categories":null,"content":"","date":1357661146,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"93e8311bbf36344de111d9ccb72d3996","permalink":"https://scott-love.github.io/fr/publication/maurage2013/","publishdate":"2013-01-08T17:05:46+01:00","relpermalink":"/fr/publication/maurage2013/","section":"publication","summary":"Face–voice integration has been extensively explored among healthy participants during the last decades. Nevertheless, while binding alterations constitute a core feature of many psychiatric diseases, these crossmodal processing have been very little explored in these populations. This chapter presents three studies offering an integrative use of behavioural, electrophysiological and neuroimaging techniques to explore the audio–visual integration of emotional stimuli in alcohol dependence. These results constitute a preliminary step towards a multidisciplinary exploration of crossmodal processing in psychiatry, extending to other stimulations, sensorial modalities and populations. The exploration of impaired crossmodal abilities could renew the knowledge on “normal” audio–visual integration and could lead to innovative therapeutic programs.","tags":[],"title":"Crossmodal Integration of Emotional Stimuli in Alcohol Dependence","type":"publication"},{"authors":["Scott A. Love","Frank E. Pollick","Karin Petrini"],"categories":null,"content":"","date":1328717424,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"ae8117fc2b068f226b357a9eafd549e7","permalink":"https://scott-love.github.io/fr/publication/love2012/","publishdate":"2012-02-08T17:10:24+01:00","relpermalink":"/fr/publication/love2012/","section":"publication","summary":"The ability to successfully integrate information from different senses is of paramount importance for perceiving the world and has been shown to change with experience. We first review how experience, in particular musical experience, brings about changes in our ability to fuse together sensory information about the world. We next discuss evidence from drumming studies that demonstrate how the perception of audiovisual synchrony depends on experience. These studies show that drummers are more robust than novices to perturbations of the audiovisual signals and appear to use different neural mechanisms in fusing sight and sound. Finally, we examine how experience influences audiovisual speech perception. We present an experiment investigating how perceiving an unfamiliar language influences judgments of temporal synchrony of the audiovisual speech signal. These results highlight the influence of both the listener’s experience with hearing an unfamiliar language as well as the speaker’s experience with producing non-native words.","tags":[],"title":"Effects of Experience, Training and Expertise on Multisensory Perception: Investigating the Link between Brain and Behavior","type":"publication"},{"authors":["Scott A. Love","Frank E. Pollick","Marianne Latinus"],"categories":null,"content":"","date":1297181595,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"dfab2064e0066c93486df6f68d864e5e","permalink":"https://scott-love.github.io/fr/publication/love2011/","publishdate":"2011-02-08T17:13:15+01:00","relpermalink":"/fr/publication/love2011/","section":"publication","summary":"Perception of faces and voices plays a prominent role in human social interaction, making multisensory integration of cross-modal speech a topic of great interest in cognitive neuroscience. How to define potential sites of multisensory integration using functional magnetic resonance imaging (fMRI) is currently under debate, with three statistical criteria frequently used (e.g., super-additive, max and mean criteria). In the present fMRI study, 20 participants were scanned in a block design under three stimulus conditions: dynamic unimodal face, unimodal voice and bimodal face–voice. Using this single dataset, we examine all these statistical criteria in an attempt to define loci of face–voice integration. While the super-additive and mean criteria essentially revealed regions in which one of the unimodal responses was a deactivation, the max criterion appeared stringent and only highlighted the left hippocampus as a potential site of face– voice integration. Psychophysiological interaction analysis showed that connectivity between occipital and temporal cortices increased during bimodal compared to unimodal conditions. We concluded that, when investigating multisensory integration with fMRI, all these criteria should be used in conjunction with manipulation of stimulus signal-to-noise ratio and/or cross-modal congruency.","tags":[],"title":"Cerebral Correlates and Statistical Criteria of Cross-Modal Face and Voice Integration","type":"publication"},{"authors":["Karin Petrini","Frank E. Pollick","Sofia Dahl","Phil McAleer","Lawrie S McKay","Davide Rocchesso","Carl Haakon Waadeland","Scott A. Love","Federico Avanzini","Aina Puce"],"categories":null,"content":"","date":1294503335,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1561131254,"objectID":"a8d3aaf21452a6d0f11bbc12e9dd197b","permalink":"https://scott-love.github.io/fr/publication/petrini2011/","publishdate":"2011-01-08T17:15:35+01:00","relpermalink":"/fr/publication/petrini2011/","section":"publication","summary":"When we observe someone perform a familiar action, we can usually predict what kind of sound that action will produce. Musical actions are over-experienced by musicians and not by non-musicians, and thus offer a unique way to examine how action expertise affects brain processes when the predictability of the produced sound is manipulated. We used functional magnetic resonance imaging to scan 11 drummers and 11 age- and gender-matched novices who made judgments on point-light drumming movements presented with sound. In Experiment 1, sound was synchronized or desynchronized with drumming strikes, while in Experiment 2 sound was always synchronized, but the natural covariation between sound intensity and velocity of the drumming strike was maintained or eliminated. Prior to MRI scanning, each participant completed psychophysical testing to identify personal levels of synchronous and asynchronous timing to be used in the two fMRI activation tasks. In both experiments, the drummers' brain activation was reduced in motor and action representation brain regions when sound matched the observed movements, and was similar to that of novices when sound was mismatched. This reduction in neural activity occurred bilaterally in the cerebellum and left parahippocampal gyrus in Experiment 1, and in the right inferior parietal lobule, inferior temporal gyrus, middle frontal gyrus and precentral gyrus in Experiment 2. Our results indicate that brain functions in action-sound representation areas are modulated by multimodal action expertise.","tags":[],"title":"Action expertise reduces brain activity for audiovisual matching actions: an fMRI study with expert drummers","type":"publication"}]