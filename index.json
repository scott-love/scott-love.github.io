[{"authors":null,"categories":null,"content":"About I\u0026rsquo;m a cognitive neuroscientist currently using neuroimaging techniques in farm animals to investigate: 1) how early environment influences behavioural and neurobiological development and 2) social perception and its underlying neural mechanisms.\nSince November 2017, I have been a researcher (Chargé de Recherche) at the PRC (Physiologie de la Reproduction et des Comportements) research unit in Nouzilly, France.\nAlso check out the INRAE Farm Animal Cognition and Welfare network that I help to animate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"00ce5bf736bf5b73096e222f166bef2e","permalink":"https://scott-love.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"About I\u0026rsquo;m a cognitive neuroscientist currently using neuroimaging techniques in farm animals to investigate: 1) how early environment influences behavioural and neurobiological development and 2) social perception and its underlying neural mechanisms.","tags":null,"title":"","type":"authors"},{"authors":["Camille Pluchot","Hans Adriaensen","Céline Parias","Didier Dubreuil","Cécile Arnould","Elodie Chaillou","Scott A. Love"],"categories":null,"content":"","date":1716895130,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716895130,"objectID":"65c877cc5d14b80c075caa3582b0fd57","permalink":"https://scott-love.github.io/publication/pluchot2024/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/pluchot2024/","section":"publication","summary":"Magnetic resonance imaging (MRI) is a non-invasive technique that requires the participant to be completely motionless. To date, MRI in awake and unrestrained animals has only been achieved with humans and dogs. For other species, alternative techniques such as anesthesia, restraint and/or sedation have been necessary. Anatomical and functional MRI studies with sheep have only been conducted under general anesthesia. This ensures the absence of movement and allows relatively long MRI experiments but it removes the non-invasive nature of the MRI technique (i.e., IV injections, intubation). Anesthesia can also be detrimental to health, disrupt neurovascular coupling, and does not permit the study of higher-level cognition. Here, we present a proof-of-concept that sheep can be trained to perform a series of tasks, enabling them to voluntarily participate in MRI sessions without anesthesia or restraint. We describe a step-by-step training protocol based on positive reinforcement (food and praise) that could be used as a basis for future neuroimaging research in sheep. This protocol details the two successive phases required for sheep to successfully achieve MRI acquisitions of their brain. By providing structural brain MRI images from six out of ten sheep, we demonstrate the feasibility of our training protocol. This innovative training protocol paves the way for the possibility of conducting animal welfare-friendly functional MRI studies with sheep to investigate ovine cognition.","tags":["sheep","mri"],"title":"Sheep (Ovis aries) training protocol for voluntary awake and unrestrained structural brain MRI acquisitions","type":"publication"},{"authors":["Aline Bertin","Baptiste Mulot","Raymond Nowak","Marie-Claire Blache","Scott Love","Mathilde Arnold","Annabelle Pinateau","Cécile Arnould","Léa Lansade"],"categories":null,"content":"","date":1674314601,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674314601,"objectID":"04eafad171cc4dbd6cf0f32671296409","permalink":"https://scott-love.github.io/publication/bertin2023/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bertin2023/","section":"publication","summary":"In mammals, human-animal bonding is recognized as a source of positive affect for companion or farm animals. Because this remains unexplored in birds, we investigated captive parrots’ perspective of the human-animal relationship. We used a classical separation-reunion paradigm and predicted that variations in parrots ’ facial displays and behaviours would indicate their appraisal of the relationship. The test was divided into three phases of two minutes each: the bird was placed in an unfamiliar environment with a familiar caregiver (union), then the bird was left alone (separation) and finally, the caregiver returned (reunion). The test was repeated 10 times for each bird and video recorded in order to analyze their behaviour. The data show significantly higher crown and nape feather heights, higher redness of the skin and higher frequency of contact-seeking behaviours during the union and reunion phases than during the separation phase during which they expressed long distance contact calls. We observed the expression of eye pinning during the union and reunion phases in one out of five macaws. We argue that variation in facial displays provides indicators of parrot ’s positive appraisal of the caretaker presence. Our results broaden the scope for further studies on parrots’ expression of their subjective feelings.","tags":["emotion"],"title":"Captive Blue-and-yellow macaws (Ara ararauna) show facial indicators of positive affect when reunited with their caregiver","type":"publication"},{"authors":["Artur Agaronyan","Raeyan Syed","Ryan Kim","Chao-Hsiung Hsu","Scott A. Love","Jacob M. Hooker","Alicia E. Reid","Paul C. Wang","Nobuyuki Ishibashi","Yeona Kang","Tsang-Wei Tu"],"categories":null,"content":"","date":1642173801,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642173801,"objectID":"7df7423bdcec79c0daab739a7a5e90ba","permalink":"https://scott-love.github.io/publication/agaronyan2021/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/agaronyan2021/","section":"publication","summary":"The olive baboon (Papio anubis) is phylogenetically proximal to humans. Investigation into the baboon brain has shed light on the function and organization of the human brain, as well as on the mechanistic insights of neurological disorders such as Alzheimer’s and Parkinson’s. Non-invasive brain imaging, including positron emission tomography (PET) and magnetic resonance imaging (MRI), are the primary outcome measures frequently used in baboon studies. PET functional imaging has long been used to study cerebral metabolic processes, though it lacks clear and reliable anatomical information. In contrast, MRI provides a clear definition of soft tissue with high resolution and contrast to distinguish brain pathology and anatomy, but lacks specific markers of neuroreceptors and/or neurometabolites. There is a need to create a brain atlas that combines the anatomical and functional/neurochemical data independently available from MRI and PET. For this purpose, a three-dimensional atlas of the olive baboon brain was developed to enable multimodal imaging analysis. The atlas was created on a population-representative template encompassing 89 baboon brains. The atlas defines 24 brain regions, including the thalamus, cerebral cortex, putamen, corpus callosum, and insula. The atlas was evaluated with four MRI images and 20 PET images employing the radiotracers for [11C]benzamide, [11C]metergoline, [18F]FAHA, and [11C]rolipram, with and without structural aids like [18F]flurodeoxyglycose images. The atlas-based analysis pipeline includes automated segmentation, registration, quantification of region volume, the volume of distribution, and standardized uptake value. Results showed that, in comparison to PET analysis utilizing the “gold standard” manual quantification by neuroscientists, the performance of the atlas-based analysis was at \u003e80 and \u003e70% agreement for MRI and PET, respectively. The atlas can serve as a foundation for further refinement, and incorporation into a high-throughput workflow of baboon PET and MRI data. The new atlas is freely available on the Figshare online repository (https://doi.org/10.6084/m9.figshare.16663339), and the template images are available from neuroImaging tools \u0026 resources collaboratory (NITRC) (https://www.nitrc.org/projects/haiko89/).","tags":["brain","anatomy","baboon","template","segmentation"],"title":"A Baboon Brain Atlas for Magnetic Resonance Imaging and Positron Emission Tomography Image Analysis","type":"publication"},{"authors":null,"categories":null,"content":"The project, Functional Neuroimaging of the Vocalisation Perception Mechanisms of Sheep (SheepVoicefMRI), is funded by the French National Research Agency - ANR-20-CE20-0001.\n","date":1518109580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518109580,"objectID":"94e2540f0149bc62d7b399103fabab32","permalink":"https://scott-love.github.io/project/sheepvoice/","publishdate":"2018-02-08T18:06:20+01:00","relpermalink":"/project/sheepvoice/","section":"project","summary":"Investigating the sheep auditory cortex with fMRI","tags":["sheep","mri","fmri","brain","voice","social"],"title":"Sheep Voice fMRI","type":"project"},{"authors":null,"categories":null,"content":"A quick(ish) tutorial on how I setup my personal but work related website using Git(Hub), Hugo and Academic on a Mac. This is a fairly simple and completely free way to host an academic personal website. The process came from two main sources: 1. The documentation for Academic and; 2. A blog post by George Cushen.\nSo what did I do: I already had Git installed and you will need it too.\nThen install Hugo. There are several options but I used the Tarball method. Full details can be found here but here is a brief description.\nChose install location (e.g., /usr/local/bin), in your executable PATH. Download the latest Tarball for your system to the Downloads folder. Install Hugo in your chosen location. cd /usr/local/bin # CHOSEN_INSTALL_LOCATION # extract the tarball tar -xvzf ~/Downloads/hugo_X.Y_osx-64bit.tgz # verify that it runs ./hugo version Fork the Academic Kickstart repository to your GitHub account.\nLogin to your GitHub account. Click here or search for sourcethemes/academic-kickstart inside GitHub. Click on the fork icon towards the top right of the screen. Clone the repository onto your local system. Note that you need to change \u0026lt;USERNAME\u0026gt; to your GitHub username and that My_Website can be changed to any name you want. git clone https://github.com/\u0026lt;USERNAME\u0026gt;/academic-kickstart.git My_Website Initialise the theme. cd My_Website git submodule update --init --recursive Create your GitHub website repository. On your GitHub page click the “+” icon in the top right corner and choose “New Repository”. Repository name = \u0026lt;USERNAME\u0026gt;.github.io. Add your \u0026lt;USERNAME\u0026gt;.github.io repository into a submodule folder named public. git submodule add https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public Add to the local git repository then push to the remote repository. git add . git commit -m \u0026quot;initial commit\u0026quot; git push -u origin master Run Hugo to create the HTML for the site. hugo server View the built site in your browser (http://localhost:1313/) but note this is only a local copy and not visible to others.\nUpload the built site to Github for everyone to see @ \u0026lt;USERNAME\u0026gt;.github.io. Note it will take 2 or 3 minutes to be viewable.\ngit add . git commit -m \u0026quot;build website\u0026quot; git push -u origin master Adding content: OK, the website is built it\u0026rsquo;s time to add content. You can make additions/changes to your website and check out the results before deploying to your actual site.\nA good place to start is the config.toml file in the main directory of your site. It contains a bunch of key-value pairs. change the title value to the title of your website, e.g., your name. Then run Hugo server to create the HTML for the site.\nhugo server View the local copy of the built site in your browser (http://localhost:1313/). Leave the server running and any other changes you make will be automatically visible in local site just created. Go through all the key-value pairs and change, edit or remove any that you want.\nGithub version control: The Host on GitHub page from Hugo outlines a nice way to store all the files of your site on GitHub.\nUpdating the website content: Using the GitHub method above I do not keep a local copy of any of the files. Here is the process I follow to update the website content.\ngit clone https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;YOUR-PROJECT\u0026gt; cd \u0026lt;YOUR-PROJECT\u0026gt;/ git rm -r public At this point you can make changes to the content of your website and push those to the remote repository.\ngit add . git commit -m \u0026quot;updating content\u0026quot; git push origin master However, this only keeps the content in synchrony with the remote repository it does not update the website.\ngit submodule add https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public ./deploy.sh \u0026quot;comment\u0026quot; The contents of deploy.sh can be seen here\nUpdate academic-kickstart version: I have found updating the academic-kickstart version and subsequently the website to be tricky! Follow the instructions here. However, I was only ever able to update using the ZIP there. Uninstall the current version by deleting the contents of the \u0026ldquo;themes/academic/\u0026rdquo; folder inside \u0026lt;YOUR-PROJECT\u0026gt; and replacing it with the downloaded files.\nAt this point you still need to follow the release notes and update your content and config files to take into account the \u0026ldquo;Breaking changes\u0026rdquo;. If you have jumped a few versions you will need to do this in sequence for each version change (e.g. v3.1 to v3.2 before doing v3.2 to v3.3)\n","date":1518024697,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518024697,"objectID":"33d571ee6822f89daf3f4badd846a8de","permalink":"https://scott-love.github.io/post/create-this-site/","publishdate":"2018-02-07T18:31:37+01:00","relpermalink":"/post/create-this-site/","section":"post","summary":"A quick(ish) tutorial on how I setup my personal but work related website using Git(Hub), Hugo and Academic on a Mac. This is a fairly simple and completely free way to host an academic personal website.","tags":null,"title":"Git(Hub), Hugo and Academic","type":"post"},{"authors":["Maureen Fontaine","Scott A. Love","Marianne Latinus"],"categories":null,"content":"","date":1486484582,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486484582,"objectID":"f7fc95c9ad416177963c6d269ce23a75","permalink":"https://scott-love.github.io/publication/fontaine2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/fontaine2017/","section":"publication","summary":"The ability to recognize an individual from their voice is a widespread ability with a long evolutionary history. Yet, the perceptual representation of familiar voices is ill-defined. In two experiments, we explored the neuropsychological processes involved in the perception of voice identity. We specifically explored the hypothesis that familiar voices (trained-to-familiar (Experiment 1), and famous voices (Experiment 2)) are represented as a whole complex pattern, well approximated by the average of multiple utterances produced by a single speaker. In experiment 1, participants learned three voices over several sessions, and performed a three-alternative forced-choice identification task on original voice samples and several “speaker averages,” created by morphing across varying numbers of different vowels (e.g., [a] and [i]) produced by the same speaker. In experiment 2, the same participants performed the same task on voice samples produced by familiar speakers. The two experiments showed that for famous voices, but not for trained-to-familiar voices, identification performance increased and response times decreased as a function of the number of utterances in the averages. This study sheds light on the perceptual representation of familiar voices, and demonstrates the power of average in recognizing familiar voices. The speaker average captures the unique characteristics of a speaker, and thus retains the information essential for recognition; it acts as a prototype of the speaker.","tags":["voice","identity","experience"],"title":"Familiarity and Voice Representation: From Acoustic-Based Representation to Voice Averages","type":"publication"},{"authors":null,"categories":null,"content":"See this poster on Figshare for some more details.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"c97427401757157a2f237781e1a58190","permalink":"https://scott-love.github.io/project/sheep-cortex/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/sheep-cortex/","section":"project","summary":"See this poster on Figshare for some more details.","tags":["sheep","anatomy","cortex","mri"],"title":"Sheep Cortex","type":"project"},{"authors":["Christophe Destrieux","Louis Marie Terrier","Frédéric Andersson","Scott A. Love","Jean-Philippe Cottier","Henri Duvernoy","Stéphane Velut","Kevin Janot","Ilyess Zemmoura"],"categories":null,"content":"","date":1454936810,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454936810,"objectID":"5fb3425c0b87148996d4c81795fcf733","permalink":"https://scott-love.github.io/publication/destrieux2016/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/destrieux2016/","section":"publication","summary":"The precise sulcogyral localization of cortical lesions is mandatory to improve communication between practitioners and to predict and prevent post-operative deficits. This process, which assumes a good knowledge of the cortex anatomy and a systematic analysis of images, is, nevertheless, sometimes neglected in the neurological and neurosurgical training. This didactic paper proposes a brief overview of the sulcogyral anatomy, using conventional MR-slices, and also reconstructions of the cortical surface after a more or less extended inflation process. This method simplifies the cortical anatomy by removing part of the cortical complexity induced by the folding process, and makes it more understandable. We then reviewed several methods for localizing cortical structures, and proposed a three-step identification: after localizing the lateral, medial or ventro-basal aspect of the hemisphere (step 1), the main interlobar sulci were located to limit the lobes (step 2). Finally, intralobar sulci and gyri were identified (step 3) thanks to the same set of rules. This paper does not propose any new identification method but should be regarded as a set of practical guidelines, useful in daily clinical practice, for detecting the main sulci and gyri of the human cortex.","tags":["anatomy","brain","mri","cortex"],"title":"A practical guide for the identification of major sulcogyral structures of the human cortex","type":"publication"},{"authors":["Marianne Latinus","Scott A. Love","Alejandra Rossi","Francisco J. Parada","Lisa Huang","Laurence Conty","Nathalie George","Karin James","Aina Puce"],"categories":null,"content":"","date":1423409366,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1423409366,"objectID":"ff269b3417d9a81027d2534165060214","permalink":"https://scott-love.github.io/publication/latinus2015/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/latinus2015/","section":"publication","summary":"Gaze direction, a cue of both social and spatial attention, is known to modulate early neural responses to faces e.g. N170. However, findings in the literature have been inconsistent, likely reflecting differences in stimulus characteristics and task requirements. Here, we investigated the effect of task on neural responses to dynamic gaze changes: away and toward transitions (resulting or not in eye contact). Subjects performed, in random order, social (away/toward them) and non-social (left/right) judgment tasks on these stimuli. Overall, in the non-social task, results showed a larger N170 to gaze aversion than gaze motion toward the observer. In the social task, however, this difference was no longer present in the right hemisphere, likely reflecting an enhanced N170 to gaze motion toward the observer. Our behavioral and event-related potential data indicate that performing social judgments enhances saliency of gaze motion toward the observer, even those that did not result in gaze contact. These data and that of previous studies suggest two modes of processing visual information: a 'default mode' that may focus on spatial information; a 'socially aware mode' that might be activated when subjects are required to make social judgments. The exact mechanism that allows switching from one mode to the other remains to be clarified.","tags":["gaze","eeg","face","social"],"title":"Social decisions affect neural activity to perceived dynamic gaze","type":"publication"},{"authors":["Corinne Jola","Phil McAleer","Marie-Hélène Grosbras","Scott A. Love","Gordon Morison","Frank E. Pollick"],"categories":null,"content":"","date":1360338582,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1360338582,"objectID":"023dc78a52c5983c97646d75b6212c88","permalink":"https://scott-love.github.io/publication/jola2013/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/jola2013/","section":"publication","summary":"The superior temporal sulcus (STS) and gyrus (STG) are commonly identified to be functionally relevant for multisensory integration of audiovisual (AV) stimuli. However, most neuroimaging studies on AV integration used stimuli of short duration in explicit evaluative tasks. Importantly though, many of our AV experiences are of a long duration and ambiguous. It is unclear if the enhanced activity in audio, visual, and AV brain areas would also be synchronised over time across subjects when they are exposed to such multisensory stimuli. We used intersubject correlation to investigate which brain areas are synchronised across novices for uni- and multisensory versions of a 6-min 26-s recording of an unfamiliar, unedited Indian dance recording (Bharatanatyam). In Bharatanatyam, music and dance are choreographed together in a highly intermodal-dependent manner. Activity in the middle and posterior STG was significantly correlated between subjects and showed also significant enhancement for AV integration when the functional magnetic resonance signals were contrasted against each other using a general linear model conjunction analysis. These results extend previous studies by showing an intermediate step of synchronisation for novices: while there was a consensus across subjects' brain activity in areas relevant for unisensory processing and AV integration of related audio and visual stimuli, we found no evidence for synchronisation of higher level cognitive processes, suggesting these were idiosyncratic.","tags":["dance","fmri","multisensory"],"title":"Uni- and multisensory brain areas are synchronised across spectators when watching unedited dance recordings","type":"publication"},{"authors":["Scott A. Love","Frank E. Pollick","Marianne Latinus"],"categories":null,"content":"","date":1297181595,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1297181595,"objectID":"c8ec2d27c6f96f46afe30dbb98b9bb74","permalink":"https://scott-love.github.io/publication/love2011/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/love2011/","section":"publication","summary":"Perception of faces and voices plays a prominent role in human social interaction, making multisensory integration of cross-modal speech a topic of great interest in cognitive neuroscience. How to define potential sites of multisensory integration using functional magnetic resonance imaging (fMRI) is currently under debate, with three statistical criteria frequently used (e.g., super-additive, max and mean criteria). In the present fMRI study, 20 participants were scanned in a block design under three stimulus conditions: dynamic unimodal face, unimodal voice and bimodal face–voice. Using this single dataset, we examine all these statistical criteria in an attempt to define loci of face–voice integration. While the super-additive and mean criteria essentially revealed regions in which one of the unimodal responses was a deactivation, the max criterion appeared stringent and only highlighted the left hippocampus as a potential site of face– voice integration. Psychophysiological interaction analysis showed that connectivity between occipital and temporal cortices increased during bimodal compared to unimodal conditions. We concluded that, when investigating multisensory integration with fMRI, all these criteria should be used in conjunction with manipulation of stimulus signal-to-noise ratio and/or cross-modal congruency.","tags":["fmri","multisensory","face","voice","social","connectivity"],"title":"Cerebral Correlates and Statistical Criteria of Cross-Modal Face and Voice Integration","type":"publication"}]